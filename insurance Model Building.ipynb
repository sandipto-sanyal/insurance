{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Objective\n",
    "This is a dataset taken from <b>UCI Machine Learning</b> repository\n",
    "<br>\n",
    "Link: https://archive.ics.uci.edu/ml/datasets/Insurance+Company+Benchmark+%28COIL+2000%29\n",
    "<br>\n",
    "<b>Data Set Information:</b>\n",
    "<br>\n",
    "Information about customers consists of <font color=\"red\">86</font> variables and includes product usage data and socio-demographic data derived from zip area codes. The data was supplied by the <font color=\"red\">Dutch data mining company Sentient Machine Research</font> and is based on a real world business problem. The training set contains over <font color=\"red\">5000</font> descriptions of customers, including the information of whether or not they have a caravan insurance policy. A test set contains <font color=\"red\">4000</font> customers of whom only the organisers know if they have a caravan insurance policy.\n",
    "<br>\n",
    "<br>\n",
    "<b>Objective:</b> Our objective is to predict whether a customer will like a \"Caravan policy\"\n",
    "<br>\n",
    "## How this dataset will address our idea in Accenture Innovation contest?\n",
    "In this contest we addressed the challenge faced by Indian insurance companies to chose the right product for the right customer. This sample POC has a similar business objective where we are trying to predict whether a customer with a particular feature will like a particular product (in our case \"A Caravan Policy) or not.\n",
    "\n",
    "## Approach to the problem\n",
    "We will take 2 steps approach towards this problem<br>\n",
    "i) Exploratory Data Analysis to find out the relevant columns which we will be taking in as input<br>\n",
    "ii) Classification Model building and performance evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\sandipto.sanyal\\Desktop\\ML\\insurance-master\\insurance-master'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import our modified training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_modified.csv')\n",
    "test = pd.read_csv('test_modified.csv')\n",
    "test_eval = pd.read_csv('tictgts2000.txt',names=['CARAVAN'])\n",
    "test = pd.concat([test,test_eval],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining both train and test set for performing Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.concat([train,test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['MOSTYPE', 'MAANTHUI', 'MGEMOMV', 'MGEMLEEF', 'MGODRK', 'MGODPR',\n",
      "       'MGODOV', 'MGODGE', 'MRELGE', 'MRELSA', 'MFALLEEN', 'MFGEKIND',\n",
      "       'MOPLHOOG', 'MOPLMIDD', 'MOPLLAAG', 'MBERHOOG', 'MBERZELF', 'MBERBOER',\n",
      "       'MBERMIDD', 'MBERARBG', 'MBERARBO', 'MSKA', 'MSKB1', 'MSKB2', 'MSKC',\n",
      "       'MSKD', 'MHHUUR', 'MAUT1', 'MAUT2', 'MAUT0', 'MZFONDS', 'MINKM30',\n",
      "       'MINK3045', 'MINK4575', 'MINK7512', 'MINK123M', 'MINKGEM', 'MKOOPKLA',\n",
      "       'PWAPART', 'CARAVAN'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(temp_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hot Encoding\n",
    "As per the business data description Hot Encoding will be done on the following columns<br>\n",
    "i) MOSTYPE: This is a category \"Customer subtype\"<br>\n",
    "Since other categories denote ranges of Age, Percentages hence we are leaving those columns as numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.get_dummies(temp_df,drop_first=True,columns=['MOSTYPE'])\n",
    "#dividing the train and test datasets again after performing hot encoding\n",
    "train = temp_df.iloc[:5822,:]\n",
    "test = temp_df.iloc[5822:len(temp_df),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the dataframes to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(train.drop(columns=['CARAVAN']))\n",
    "y_train = np.array(train.CARAVAN)\n",
    "X_test = np.array(test.drop(columns=['CARAVAN']))\n",
    "y_test = np.array(test.CARAVAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying standard scaler to standardize the values of columns\n",
    "Standardization is an important part before feeding into ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sandipto.sanyal\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing SMOTE to reduce imbalance in training dataset\n",
    "Since the training DS is a highly imbalanced one we are performing Oversampling of minority data (people who have liked caravan policy) using SMOTE algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of CARAVAN column before SMOTE\n",
      "[[   0    1]\n",
      " [5474  348]]\n",
      "Distribution of CARAVAN column after SMOTE\n",
      "[[   0    1]\n",
      " [5474 5474]]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state = 12)\n",
    "X_res, y_res = smote.fit_sample(X_train, y_train)\n",
    "unique, counts = np.unique(y_res, return_counts=True)\n",
    "print('Distribution of CARAVAN column before SMOTE')\n",
    "print(np.asarray((np.unique(y_train,return_counts=True))))\n",
    "print('Distribution of CARAVAN column after SMOTE')\n",
    "print(np.asarray((unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(choice):\n",
    "    if choice == 1:\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        classifier = LogisticRegression(random_state = 12)\n",
    "    elif choice == 2:\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        classifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\n",
    "    elif choice == 3:\n",
    "        from sklearn.naive_bayes import GaussianNB\n",
    "        classifier = GaussianNB()\n",
    "    elif choice == 4:\n",
    "        classifier = ANN()\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN():\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation\n",
    "    model = Sequential()\n",
    "    #input layer\n",
    "    model.add(Dense(units=38,activation='relu',input_shape=(X_res.shape[1],)))\n",
    "    #hidden layer\n",
    "    model.add(Dense(units=38,activation='relu'))\n",
    "    #output layer\n",
    "    model.add(Dense(units=1,activation='sigmoid'))\n",
    "    #compile\n",
    "    model.compile(\n",
    "     optimizer = \"adam\",\n",
    "     loss = \"binary_crossentropy\",\n",
    "     metrics = [\"accuracy\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-1c0ffdba4e55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mchoice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#fit = classifier(choice).fit(X_res,y_res)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m fit = classifier(choice).fit(X_res,y_res,\n\u001b[0m\u001b[0;32m      4\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                             \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-1d589f1ad84e>\u001b[0m in \u001b[0;36mclassifier\u001b[1;34m(choice)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mANN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-b85d39a52aec>\u001b[0m in \u001b[0;36mANN\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mANN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#input layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "choice = 4\n",
    "#fit = classifier(choice).fit(X_res,y_res)\n",
    "fit = classifier(choice).fit(X_res,y_res,\n",
    "                            epochs=2,\n",
    "                            batch_size = 500,\n",
    "                            validation_data = (X_test, y_test)\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = fit.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metrics\n",
    "Here we evaluate the performances of different models<br>\n",
    "We will check the following metrics:<br>\n",
    "i) Accuracy: Check the fraction of correct predictions in the total test data set.<br>\n",
    "ii) Precision: Out of total positive predictions that customer will like the product how many were actually correct<br>\n",
    "iii) Recall: Out of total positives that customer liked the product in the test set how many did our model correctly predict.<br>\n",
    "iv) F1 score: Harmonic mean of both Precision and Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "y_test1 = y_test\n",
    "cm = confusion_matrix(y_test1, y_pred)\n",
    "print('Confusion matrix:\\n' + str(cm))\n",
    "print('Accuracy_score: %.2f' %(accuracy_score(y_test1, y_pred)))\n",
    "print('Precision_score: %.2f' %(precision_score(y_test1, y_pred))) # tp / (tp + fp)\n",
    "print('Recall_score: %.2f' %(recall_score(y_test1, y_pred))) # tp / (tp + fn)\n",
    "print('f1_score: %.2f' %(f1_score(y_test1, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_pred, return_counts=True)\n",
    "print('Prediction distribution')\n",
    "print(np.asarray((unique, counts)))\n",
    "\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "print('Validation distribution')\n",
    "print(np.asarray((unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
